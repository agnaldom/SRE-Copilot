# Testes de ConexÃ£o com Datadog

Este diretÃ³rio contÃ©m uma suÃ­te abrangente de testes para validar a integraÃ§Ã£o com o Datadog no SRE Copilot.

## ğŸ“‹ VisÃ£o Geral

Os testes cobrem todos os aspectos da conexÃ£o com Datadog:
- âœ… **Testes UnitÃ¡rios**: ValidaÃ§Ã£o de funcionalidades individuais
- âš¡ **Testes de Performance**: ValidaÃ§Ã£o de performance e escalabilidade  
- ğŸ”— **Testes de IntegraÃ§Ã£o**: ValidaÃ§Ã£o de fluxos completos
- ğŸš€ **Testes de Stress**: ValidaÃ§Ã£o sob carga alta

## ğŸ“ Estrutura dos Arquivos

```
tests/
â”œâ”€â”€ README.md                        # Esta documentaÃ§Ã£o
â”œâ”€â”€ conftest.py                      # ConfiguraÃ§Ãµes globais e fixtures
â”œâ”€â”€ test_datadog_connection.py       # Testes principais de conexÃ£o
â””â”€â”€ test_datadog_performance.py      # Testes de performance e stress
```

## ğŸ§ª Categorias de Testes

### 1. Testes de ConexÃ£o (`test_datadog_connection.py`)

#### `TestDatadogConnection`
- âœ… InicializaÃ§Ã£o bem-sucedida com credenciais vÃ¡lidas
- âŒ Falha na inicializaÃ§Ã£o com credenciais ausentes
- ğŸ”§ ConfiguraÃ§Ã£o correta da API client

#### `TestDatadogMonitors`
- ğŸ“Š Busca de monitores ativos (P1/P2)
- ğŸ·ï¸ Filtragem por tags
- ğŸ¯ Filtragem por prioridade
- ğŸš¨ Tratamento de erros da API
- ğŸ“­ Tratamento de resultados vazios

#### `TestDatadogLogs`
- ğŸ“ Busca de logs com parÃ¢metros padrÃ£o
- âš™ï¸ Busca de logs com parÃ¢metros customizados
- âŒ Tratamento de erros na API de logs
- ğŸ”„ Compatibilidade com diferentes formatos de resposta

#### `TestDatadogGeneralOperations`
- ğŸ” Processamento de queries string e JSON
- â“ Tratamento de aÃ§Ãµes desconhecidas
- ğŸš« Funcionalidades nÃ£o implementadas
- ğŸ’¥ Tratamento de exceÃ§Ãµes gerais

#### `TestDatadogIntegration`
- ğŸ”„ Fluxo completo end-to-end
- â° SimulaÃ§Ã£o de timeouts

### 2. Testes de Performance (`test_datadog_performance.py`)

#### `TestDatadogPerformance`
- ğŸ“ˆ **Teste de Dataset Grande**: 1000+ monitores
- ğŸ”€ **Testes Concorrentes**: 10 requisiÃ§Ãµes simultÃ¢neas
- ğŸ’¾ **Teste de MemÃ³ria**: Monitoramento de vazamentos
- â±ï¸ **Teste de Timeout**: Tratamento de respostas lentas

#### `TestDatadogRateLimiting`
- ğŸš¦ Tratamento de rate limiting (429)
- ğŸ’¨ Testes de requisiÃ§Ãµes em burst

#### `TestDatadogErrorScenarios`
- ğŸ” Erro de autenticaÃ§Ã£o (401)
- ğŸš« Erro de permissÃ£o (403)
- ğŸš§ ServiÃ§o indisponÃ­vel (503)
- ğŸ“‹ Resposta malformada da API
- ğŸŒ Erro de conexÃ£o de rede
- ğŸ”§ Erro de decodificaÃ§Ã£o JSON

#### `TestDatadogStressTest`
- ğŸ”„ **Estabilidade**: 100 requisiÃ§Ãµes repetidas
- ğŸ§  **DetecÃ§Ã£o de Memory Leak**: 50 instÃ¢ncias criadas/destruÃ­das

## ğŸ¯ Markers de Teste

Os testes utilizam markers para categorizaÃ§Ã£o:

- `@pytest.mark.unit` - Testes unitÃ¡rios bÃ¡sicos
- `@pytest.mark.integration` - Testes de integraÃ§Ã£o
- `@pytest.mark.performance` - Testes de performance
- `@pytest.mark.slow` - Testes que demoram >5 segundos
- `@pytest.mark.datadog` - EspecÃ­ficos do Datadog

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Script Automatizado (Recomendado)

```bash
# Verificar ambiente
python run_datadog_tests.py --check

# Instalar dependÃªncias
python run_datadog_tests.py --install

# Testes rÃ¡pidos (recomendado para desenvolvimento)
python run_datadog_tests.py --quick

# Todos os testes
python run_datadog_tests.py --all

# Testes de performance
python run_datadog_tests.py --performance

# Gerar relatÃ³rio completo
python run_datadog_tests.py --report
```

### OpÃ§Ã£o 2: pytest Diretamente

```bash
# Todos os testes do Datadog
pytest tests/test_datadog_connection.py tests/test_datadog_performance.py -v

# Apenas testes unitÃ¡rios
pytest tests/test_datadog_connection.py -m "not (performance or slow)" -v

# Apenas testes de performance
pytest tests/test_datadog_performance.py -m "performance" -v

# Com cobertura de cÃ³digo
pytest tests/ --cov=tools --cov-report=html
```

### OpÃ§Ã£o 3: Testes EspecÃ­ficos

```bash
# Testar apenas inicializaÃ§Ã£o
pytest tests/test_datadog_connection.py::TestDatadogConnection -v

# Testar apenas monitores
pytest tests/test_datadog_connection.py::TestDatadogMonitors -v

# Testar performance de monitores grandes
pytest tests/test_datadog_performance.py::TestDatadogPerformance::test_get_monitors_large_dataset -v
```

## ğŸ“Š Cobertura de CÃ³digo

Os testes incluem mediÃ§Ã£o de cobertura de cÃ³digo:

```bash
# Gerar relatÃ³rio de cobertura
pytest tests/ --cov=tools --cov-report=html:htmlcov --cov-report=term-missing

# Visualizar relatÃ³rio HTML
open htmlcov/index.html
```

Meta de cobertura: **â‰¥80%**

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente para Testes

```bash
# NÃ£o sÃ£o necessÃ¡rias para testes unitÃ¡rios (usam mocks)
# Apenas para testes de integraÃ§Ã£o reais:
export DATADOG_API_KEY="sua_api_key_aqui"
export DATADOG_APP_KEY="sua_app_key_aqui"
```

### DependÃªncias Adicionais

```bash
pip install pytest>=7.4.0 pytest-asyncio pytest-cov pytest-timeout psutil
```

## ğŸ“ˆ MÃ©tricas de Performance

### Benchmarks Esperados

| Teste | Tempo Esperado | CritÃ©rio |
|-------|----------------|----------|
| Testes UnitÃ¡rios | <2 segundos | Individual <100ms |
| Processamento 1000 monitores | <5 segundos | Sem vazamento de memÃ³ria |
| 10 requisiÃ§Ãµes concorrentes | <10 segundos | Todas devem suceder |
| 100 requisiÃ§Ãµes sequenciais | <30 segundos | â‰¥95% taxa de sucesso |

### Limites de Recursos

- **MemÃ³ria**: Aumento mÃ¡ximo de 100MB para datasets grandes
- **Timeout**: 5 minutos por teste individual
- **Rate Limiting**: Tratamento gracioso de erros 429

## ğŸ› Debugging

### Logs de Teste

```bash
# Executar com logs detalhados
pytest tests/ -v -s --log-cli-level=DEBUG

# Capturar apenas falhas
pytest tests/ --tb=short --maxfail=1
```

### Fixtures DisponÃ­veis

- `mock_env_vars`: Mock das variÃ¡veis de ambiente
- `datadog_tool`: InstÃ¢ncia mockada da ferramenta
- `sample_monitor_data`: Dados de exemplo de monitor
- `sample_logs_data`: Dados de exemplo de logs

## ğŸ¤ Contribuindo

### Adicionando Novos Testes

1. **Testes UnitÃ¡rios**: Adicione em `test_datadog_connection.py`
2. **Testes de Performance**: Adicione em `test_datadog_performance.py`
3. **Fixtures Comuns**: Adicione em `conftest.py`

### ConvenÃ§Ãµes

- Nomes de teste descritivos: `test_get_monitors_with_invalid_credentials`
- DocumentaÃ§Ã£o: Docstrings explicando o cenÃ¡rio testado
- Markers apropriados: `@pytest.mark.performance` para testes de performance
- Assertions claras: Mensagens de erro informativas

### Exemplo de Novo Teste

```python
@pytest.mark.unit
def test_get_monitors_with_custom_filter(self, datadog_tool):
    """Testa busca de monitores com filtro customizado."""
    # Setup
    mock_monitor = Mock()
    mock_monitor.id = 123
    # ... configurar mock
    
    datadog_tool.monitors_api.list_monitors.return_value = [mock_monitor]
    
    # Executar
    result = datadog_tool._run(json.dumps({
        "action": "get_monitors",
        "custom_filter": "value"
    }))
    
    # Verificar
    result_data = json.loads(result)
    assert result_data["status"] == "success"
    assert result_data["count"] == 1
```

## ğŸ“ Suporte

Para problemas com os testes:

1. Verifique o ambiente: `python run_datadog_tests.py --check`
2. Execute testes rÃ¡pidos primeiro: `python run_datadog_tests.py --quick`
3. Verifique logs detalhados: `pytest -v -s --tb=long`
4. Consulte a documentaÃ§Ã£o da API do Datadog para mudanÃ§as

---

**ğŸ“ Nota**: Estes testes usam mocks por padrÃ£o, nÃ£o fazendo chamadas reais Ã  API do Datadog. Para testes de integraÃ§Ã£o reais, configure as variÃ¡veis de ambiente apropriadas.
